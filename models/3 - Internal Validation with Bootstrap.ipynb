{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 -  Internal Validation with Boostrap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Jupyter notebook contains the code to load the models obtained in the generation step, and then evaluate the MCAR (Multiple Completely at Random) condition, generate multiple imputations, and finally use bootstrap sampling for internal validation including optimism-corrected estimates of the performance parameters (AUC, accuracy, sensitivity, specificity, etc) including 95% Confidence Intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The rpy2.ipython extension is already loaded. To reload it, use:\n",
      "  %reload_ext rpy2.ipython\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "# pipelining with R\n",
    "# https://blog.dominodatalab.com/lesser-known-ways-of-using-notebooks/\n",
    "\n",
    "# Use the RWinOut instead of rpy2.ipython to get output on windows \n",
    "# https://bitbucket.org/rpy2/rpy2/issues/125/set_writeconsole-not-working-on-windows\n",
    "# https://github.com/vitorcurtis/RWinOut\n",
    "#%load_ext RWinOut\n",
    "%load_ext rpy2.ipython\n",
    "\n",
    "%R library(Hmisc)\n",
    "%R library(mice)\n",
    "%R library(boot)\n",
    "%R library(rms)\n",
    "%R library(ROCR)\n",
    "%R library(ResourceSelection)\n",
    "%R library(LogisticDx)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load import_notebook.py\n",
    "# Infraestructure to import a Jupyter notebook\n",
    "# http://jupyter-notebook.readthedocs.io/en/latest/examples/Notebook/Importing%20Notebooks.html\n",
    "\n",
    "import io, os, sys, types\n",
    "from IPython import get_ipython\n",
    "from nbformat import read\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "def find_notebook(fullname, path=None):\n",
    "    \"\"\"find a notebook, given its fully qualified name and an optional path\n",
    "\n",
    "    This turns \"foo.bar\" into \"foo/bar.ipynb\"\n",
    "    and tries turning \"Foo_Bar\" into \"Foo Bar\" if Foo_Bar\n",
    "    does not exist.\n",
    "    \"\"\"\n",
    "    name = fullname.rsplit('.', 1)[-1]\n",
    "    if not path:\n",
    "        path = ['']\n",
    "    for d in path:\n",
    "        nb_path = os.path.join(d, name + \".ipynb\")\n",
    "        if os.path.isfile(nb_path):\n",
    "            return nb_path\n",
    "        # let import Notebook_Name find \"Notebook Name.ipynb\"\n",
    "        nb_path = nb_path.replace(\"_\", \" \")\n",
    "        if os.path.isfile(nb_path):\n",
    "            return nb_path\n",
    "        \n",
    "\n",
    "class NotebookLoader(object):\n",
    "    \"\"\"Module Loader for Jupyter Notebooks\"\"\"\n",
    "    def __init__(self, path=None):\n",
    "        self.shell = InteractiveShell.instance()\n",
    "        self.path = path\n",
    "\n",
    "    def load_module(self, fullname):\n",
    "        \"\"\"import a notebook as a module\"\"\"\n",
    "        path = find_notebook(fullname, self.path)\n",
    "\n",
    "        print (\"importing Jupyter notebook from %s\" % path)\n",
    "\n",
    "        # load the notebook object\n",
    "        with io.open(path, 'r', encoding='utf-8') as f:\n",
    "            nb = read(f, 4)\n",
    "\n",
    "\n",
    "        # create the module and add it to sys.modules\n",
    "        # if name in sys.modules:\n",
    "        #    return sys.modules[name]\n",
    "        mod = types.ModuleType(fullname)\n",
    "        mod.__file__ = path\n",
    "        mod.__loader__ = self\n",
    "        mod.__dict__['get_ipython'] = get_ipython\n",
    "        sys.modules[fullname] = mod\n",
    "\n",
    "        # extra work to ensure that magics that would affect the user_ns\n",
    "        # actually affect the notebook module's ns\n",
    "        save_user_ns = self.shell.user_ns\n",
    "        self.shell.user_ns = mod.__dict__\n",
    "\n",
    "        try:\n",
    "            for cell in nb.cells:\n",
    "                if cell.cell_type == 'code':\n",
    "                    # transform the input to executable Python\n",
    "                    code = self.shell.input_transformer_manager.transform_cell(cell.source)\n",
    "                    # run the code in themodule\n",
    "                    exec(code, mod.__dict__)\n",
    "        finally:\n",
    "            self.shell.user_ns = save_user_ns\n",
    "        return mod\n",
    "    \n",
    "class NotebookFinder(object):\n",
    "    \"\"\"Module finder that locates Jupyter Notebooks\"\"\"\n",
    "    def __init__(self):\n",
    "        self.loaders = {}\n",
    "\n",
    "    def find_module(self, fullname, path=None):\n",
    "        nb_path = find_notebook(fullname, path)\n",
    "        if not nb_path:\n",
    "            return\n",
    "\n",
    "        key = path\n",
    "        if path:\n",
    "            # lists aren't hashable\n",
    "            key = os.path.sep.join(path)\n",
    "\n",
    "        if key not in self.loaders:\n",
    "            self.loaders[key] = NotebookLoader(path)\n",
    "        return self.loaders[key]\n",
    "\n",
    "sys.meta_path.append(NotebookFinder())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from LogRegUtils import LogRegModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data file and modeling prameters\n",
    "\n",
    "src_data_file = '../data/data_normalized.csv'\n",
    "\n",
    "risk_threshold = 0.5 # Classification threshold to define an outcome (1 if risk_threshold <= p, 0 otherwise)\n",
    "\n",
    "num_imp = 100   # Number of multiple imputations\n",
    "num_boot = 200  # Number of bootstrap samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel_model = 9\n",
    "    \n",
    "sl_only = False\n",
    "lb_only = False\n",
    "\n",
    "if sel_model == 1:\n",
    "    model_name = 'all-minimal'\n",
    "elif sel_model == 2:\n",
    "    model_name = 'all-clinical-only'\n",
    "elif sel_model == 3:\n",
    "    model_name = 'all-parsimonious'\n",
    "elif sel_model == 4:\n",
    "    model_name = 'all-parsimonious-notemp'    \n",
    "elif sel_model == 5:    \n",
    "    model_name = 'sl-wellness-parsimonious'\n",
    "    sl_only = True\n",
    "elif sel_model == 6:    \n",
    "    model_name = 'sl-wellness-parsimonious-notemp'\n",
    "    sl_only = True    \n",
    "elif sel_model == 7:\n",
    "    model_name = 'sl-wellness-clinical-only'\n",
    "    sl_only = True    \n",
    "elif sel_model == 8:\n",
    "    model_name = 'sl-wellness-minimal'\n",
    "    sl_only = True\n",
    "elif sel_model == 9:\n",
    "    src_data_file = '../data/data.csv'\n",
    "    model_name = 'sl-parsimonious'\n",
    "    sl_only = True\n",
    "    \n",
    "elif sel_model == 10:    \n",
    "    model_name = 'sl-wellness-parsimonious-noimp'\n",
    "    sl_only = True\n",
    "elif sel_model == 11:    \n",
    "    model_name = 'sl-wellness-parsimonious-notemp-noimp'\n",
    "    sl_only = True    \n",
    "elif sel_model == 12:\n",
    "    model_name = 'sl-wellness-clinical-only-noimp'\n",
    "    sl_only = True    \n",
    "elif sel_model == 13:\n",
    "    model_name = 'sl-wellness-minimal-noimp'\n",
    "    sl_only = True    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "yvar = 'Disposition'\n",
    "\n",
    "model_params = os.path.join(model_name, 'model.csv')\n",
    "model = LogRegModel(model_params)\n",
    "\n",
    "model_variables = [yvar] + model.getPredictors()\n",
    "\n",
    "imp_variables = model_variables.copy()\n",
    "if 'FeverTemperature' in model_variables:\n",
    "    imp_variables += ['Fever']\n",
    "\n",
    "model_formula = model.getGLMFormula(yvar)\n",
    "model_imp_formula = model.getImputeFormula(yvar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model variables:\n",
      " ['Disposition', 'PatientAge', 'cycletime', 'malaria1', 'FeverTemperature', 'Jaundice', 'Bleeding', 'Breathlessness', 'SwallowingProblems', 'AstheniaWeakness', 'DaysSinceSymptomFeverOnset'] \n",
      "\n",
      "Imputation variables:\n",
      " ['Disposition', 'PatientAge', 'cycletime', 'malaria1', 'FeverTemperature', 'Jaundice', 'Bleeding', 'Breathlessness', 'SwallowingProblems', 'AstheniaWeakness', 'DaysSinceSymptomFeverOnset', 'Fever'] \n",
      "\n",
      "Model formula:\n",
      " Disposition~rcs(PatientAge,3,c(4.0,28.0,59.8))+cycletime+malaria1+rcs(FeverTemperature,3,c(36.3,37.3,39.2))+Jaundice+Bleeding+Breathlessness+SwallowingProblems+AstheniaWeakness+DaysSinceSymptomFeverOnset+cycletime * DaysSinceSymptomFeverOnset \n",
      "\n",
      "Imputation formula:\n",
      " ~Disposition+PatientAge+cycletime+malaria1+FeverTemperature+Jaundice+Bleeding+Breathlessness+SwallowingProblems+AstheniaWeakness+DaysSinceSymptomFeverOnset \n",
      "\n",
      "Training files: 0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create model folder\n",
    "from os import listdir, makedirs\n",
    "from os.path import isfile, join, exists\n",
    "\n",
    "model_folder = model_name\n",
    "imp_folder = join(model_name, 'imp')\n",
    "if not exists(imp_folder):\n",
    "    makedirs(imp_folder)\n",
    "\n",
    "# Load imputed data files, in case they exists. This allows to re-run bootstrap \n",
    "# calculations on same training set :-)    \n",
    "imp_data_files = [join(imp_folder, f) for f in listdir(imp_folder) if isfile(join(imp_folder, f))]\n",
    "\n",
    "print('Model variables:\\n', model_variables, '\\n')\n",
    "print('Imputation variables:\\n', imp_variables, '\\n')\n",
    "print('Model formula:\\n', model_formula, '\\n')\n",
    "print('Imputation formula:\\n', model_imp_formula, '\\n')\n",
    "print('Training files:', len(imp_data_files), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R \n",
    "\n",
    "# Random seed for reproducibility. Is reset at the beginning of each \n",
    "# calculating that involves randomization, so cells can be run in \n",
    "# arbitrary order and still reproduce results.\n",
    "\n",
    "random_seed <- 151 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Iteration 1 \r",
       "Iteration 2 \r",
       "Iteration 3 \r",
       "Iteration 4 \r",
       "Iteration 5 \r",
       "Iteration 6 \r",
       "Iteration 7 \r",
       "Iteration 8 \r",
       "Iteration 9 \r",
       "Iteration 10 \r",
       "Iteration 11 \r",
       "Iteration 12 \r",
       "Iteration 13 \r",
       "Iteration 14 \r",
       "Iteration 15 \r",
       "Iteration 16 \r",
       "Iteration 17 \r",
       "Iteration 18 \r",
       "Iteration 19 \r",
       "Iteration 20 \r",
       "Iteration 21 \r",
       "Iteration 22 \r",
       "Iteration 23 \r",
       "Iteration 24 \r",
       "Iteration 25 \r",
       "Iteration 26 \r",
       "Iteration 27 \r",
       "Iteration 28 \r",
       "Iteration 29 \r",
       "Iteration 30 \r",
       "Iteration 31 \r",
       "Iteration 32 \r",
       "Iteration 33 \r",
       "Iteration 34 \r",
       "Iteration 35 \r",
       "Iteration 36 \r",
       "Iteration 37 \r",
       "Iteration 38 \r",
       "Iteration 39 \r",
       "Iteration 40 \r",
       "Iteration 41 \r",
       "Iteration 42 \r",
       "Iteration 43 \r",
       "Iteration 44 \r",
       "Iteration 45 \r",
       "Iteration 46 \r",
       "Iteration 47 \r",
       "Iteration 48 \r",
       "Iteration 49 \r",
       "Iteration 50 \r",
       "Iteration 51 \r",
       "Iteration 52 \r",
       "Iteration 53 \r",
       "Iteration 54 \r",
       "Iteration 55 \r",
       "Iteration 56 \r",
       "Iteration 57 \r",
       "Iteration 58 \r",
       "Iteration 59 \r",
       "Iteration 60 \r",
       "Iteration 61 \r",
       "Iteration 62 \r",
       "Iteration 63 \r",
       "Iteration 64 \r",
       "Iteration 65 \r",
       "Iteration 66 \r",
       "Iteration 67 \r",
       "Iteration 68 \r",
       "Iteration 69 \r",
       "Iteration 70 \r",
       "Iteration 71 \r",
       "Iteration 72 \r",
       "Iteration 73 \r",
       "Iteration 74 \r",
       "Iteration 75 \r",
       "Iteration 76 \r",
       "Iteration 77 \r",
       "Iteration 78 \r",
       "Iteration 79 \r",
       "Iteration 80 \r",
       "Iteration 81 \r",
       "Iteration 82 \r",
       "Iteration 83 \r",
       "Iteration 84 \r",
       "Iteration 85 \r",
       "Iteration 86 \r",
       "Iteration 87 \r",
       "Iteration 88 \r",
       "Iteration 89 \r",
       "Iteration 90 \r",
       "Iteration 91 \r",
       "Iteration 92 \r",
       "Iteration 93 \r",
       "Iteration 94 \r",
       "Iteration 95 \r",
       "Iteration 96 \r",
       "Iteration 97 \r",
       "Iteration 98 \r",
       "Iteration 99 \r",
       "Iteration 100 \r",
       "Iteration 101 \r",
       "Iteration 102 \r",
       "Iteration 103 \r\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%R -i num_imp,model_imp_formula,imp_variables,src_data_file,model_folder,sl_only,lb_only -o imp_data_files\n",
    "\n",
    "set.seed(random_seed)\n",
    "\n",
    "src_data <- read.table(src_data_file, sep=\",\", header=TRUE, na.strings=\"\\\\N\")\n",
    "if (sl_only) {\n",
    "    src_data <- src_data[src_data$ETUKey == 2 | src_data$ETUKey == 4 | src_data$ETUKey == 5, ]\n",
    "}\n",
    "if (lb_only) {\n",
    "    src_data <- src_data[src_data$ETUKey == 1 | src_data$ETUKey == 3, ]\n",
    "}\n",
    "sat_impute <- aregImpute(as.formula(model_imp_formula), data=src_data, n.impute=num_imp)\n",
    "\n",
    "imp_data_files <- character(0)\n",
    "for (iter in 1:num_imp) {\n",
    "  completed <- src_data\n",
    "  imputed <- impute.transcan(sat_impute, imputation=iter, data=src_data, list.out=TRUE,\n",
    "                             pr=FALSE, check=FALSE)\n",
    "  completed[names(imputed)] <- imputed\n",
    "  comp_data <- completed[unlist(imp_variables)]\n",
    "  fn <- paste0(model_folder, '/imp', \"/imputation-\", iter, \".csv\")\n",
    "  write.csv(comp_data, file=fn, row.names=FALSE)\n",
    "  imp_data_files <- c(imp_data_files, fn)    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Internal validation by bootstrap sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1] \"Apparent AUC : 0.83 (0.78, 0.87)\"\n",
       "[1] \"Corrected AUC: 0.78 (0.72, 0.83)\"\n",
       "[1] \"Adjusted R2  : 0.34 (0.27, 0.42)\"\n",
       "[1] \"Brier score  : 0.19 (0.14, 0.25)\"\n",
       "[1] \"Calibration  : 0.03 (0.01, 0.04)\"\n",
       "[1] \"Accuracy     : 0.72 (0.65, 0.78)\"\n",
       "[1] \"Sensitivity  : 0.80 (0.75, 0.85)\"\n",
       "[1] \"Specificity  : 0.62 (0.54, 0.69)\"\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%R -i num_boot,risk_threshold,yvar,model_formula,model_folder,imp_data_files\n",
    "\n",
    "set.seed(random_seed)\n",
    "\n",
    "# (Adjusted) McFadden R2\n",
    "# In the future we could use this library for calculation\n",
    "# http://www.inside-r.org/packages/cran/bayloredpsych/docs/PseudoR2        \n",
    "adjr2 <- function(obj) {\n",
    "    # For the time being, just get numer of dofs in model (including intercept) \n",
    "    # using LogLik: http://stats.stackexchange.com/a/5580\n",
    "    ll <- logLik(obj)\n",
    "    K <- attr(ll, \"df\")\n",
    "    r2 <- 1 - ((obj$deviance - K) / obj$null.deviance)\n",
    "    return(r2)\n",
    "}\n",
    "        \n",
    "calib <- function(probs,outcome,nbins=10) {\n",
    "    c <- 0.0\n",
    "\n",
    "    # Construct bins\n",
    "    judgement_bins <- seq(0, nbins)/nbins\n",
    "\n",
    "    # Which bin is each prediction in?\n",
    "    bin_num <- .bincode(probs, judgement_bins, TRUE)\n",
    "\n",
    "    for (j_bin in sort(unique(bin_num))) {\n",
    "        # Is event in bin\n",
    "        in_bin <- bin_num == j_bin\n",
    "        \n",
    "        # Predicted probability taken as average of preds in bin\n",
    "        predicted_prob <- mean(probs[in_bin])\n",
    "        \n",
    "        # How often did events in this bin actually happen?\n",
    "        true_bin_prob <- mean(outcome[in_bin])\n",
    "        \n",
    "        # Squared distance between predicted and true times num of obs\n",
    "        c <- c + sum(in_bin) * (predicted_prob - true_bin_prob)^2\n",
    "    } \n",
    "    cal <- c / length(probs)\n",
    "    return(cal)\n",
    "}\n",
    "        \n",
    "brier <- function(probs,outcome) {\n",
    "    res <- mean((probs - outcome)^2)\n",
    "    return(res)\n",
    "}\n",
    " \n",
    "accu <- function(probs,outcome) {\n",
    "    preds = risk_threshold <= probs\n",
    "    res <- 1 - mean(abs(preds - outcome))\n",
    "    return(res)\n",
    "}        \n",
    "\n",
    "sens <- function(probs,outcome) {\n",
    "    preds = risk_threshold <= probs\n",
    "    tp <- sum(preds & outcome)\n",
    "    fn <- sum(!preds & outcome)\n",
    "#     print(sprintf(\"sens : %0.3f %0.3f %0.3f\", tp, fn, tp / (tp + fn)))\n",
    "    return(tp / (tp + fn))\n",
    "}\n",
    "\n",
    "spec <- function(probs,outcome) {\n",
    "    preds = risk_threshold <= probs\n",
    "    tn <- sum(!preds & !outcome)\n",
    "    fp <- sum(preds & !outcome)\n",
    "    return(tn / (tn + fp))\n",
    "}\n",
    "\n",
    "# Transform Z-scores back to score, and calculates CI at 95%\n",
    "# https://stats.idre.ucla.edu/stata/faq/how-can-i-estimate-r-squared-for-a-model-estimated-with-multiply-imputed-data/\n",
    "# Harel, O. (2009). The estimation of R2 and adjusted R2 in incomplete data sets using multiple imputation. Journal of Applied Statistics, 36(10), 1109-1118.\n",
    "zinv <- function(values, N, M) {\n",
    "    # Fist, we need the inter-imputation variance\n",
    "    B <- sum((values - mean(values))^2) / (M - 1)\n",
    "\n",
    "    # Now, we get the MI estimate of the variance of z\n",
    "    V <- 1/(N-3) + B/(M+1)\n",
    "     \n",
    "    # The confidence interval, using the confidence level for 95%    \n",
    "    Q <- mean(values)  \n",
    "    ci_min <- tanh(Q - 1.959964*sqrt(V*Q))^2\n",
    "    ci_max <- tanh(Q + 1.959964*sqrt(V*Q))^2\n",
    "    val_mean <- tanh(Q)^2\n",
    "    \n",
    "    res <- c(val_mean, ci_min, ci_max)\n",
    "    return(res)\n",
    "}\n",
    "     \n",
    "optim <- function(src_dat, boot_idx) {\n",
    "    src_idx <- 1:nrow(src_dat)\n",
    "    boot_idx <- sample(src_idx, replace=TRUE)\n",
    "    boot_dat <- src_dat[boot_idx,]\n",
    "\n",
    "    boot_y <- as.matrix(boot_dat[,1])\n",
    "    boot_x <- as.matrix(boot_dat[,2:ncol(boot_dat)])\n",
    "  \n",
    "    boot_mod <- glm(family=\"binomial\", formula=mod_formula, data=boot_dat)\n",
    "\n",
    "    # Get the indices of the rows not used in the bootstrap sample (the .632 method)\n",
    "    rem_idx <- setdiff(src_idx, boot_idx)\n",
    "    rem_dat <- train_data[rem_idx,] \n",
    "    rem_x <- as.matrix(rem_dat[,2:ncol(rem_dat)])\n",
    "    rem_y <- as.matrix(rem_dat[,1])\n",
    "    \n",
    "    boot_prob <- predict(boot_mod, boot_dat, type=\"response\")\n",
    "    boot_pred <- prediction(boot_prob, boot_y)\n",
    "    boot_auc <- performance(boot_pred, measure = \"auc\")\n",
    "\n",
    "    rem_prob <- predict(boot_mod, rem_dat, type=\"response\")\n",
    "    rem_pred <- prediction(rem_prob, rem_y)\n",
    "    auc <- performance(rem_pred, measure = \"auc\")\n",
    "    \n",
    "    # Values should not be 1, otherwise the Z-score transformation will give infinity\n",
    "    rem_auc <- min(0.999, auc@y.values[[1]])    \n",
    "    rem_bri <- min(0.999, brier(rem_prob, rem_y))\n",
    "    rem_cal <- min(0.999, calib(rem_prob, rem_y))\n",
    "    rem_acc <- min(0.999, accu(rem_prob, rem_y))\n",
    "    rem_sens <- min(0.999, sens(rem_prob, rem_y))\n",
    "    rem_spec <- min(0.999, spec(rem_prob, rem_y))    \n",
    "    rem_r2 <- min(0.999, adjr2(boot_mod))\n",
    "\n",
    "    # All values are returned as Z-scores using the method from \n",
    "    # http://www.ats.ucla.edu/stat/stata/faq/mi_r_squared.htm    \n",
    "    auc_value <- atanh(sqrt(rem_auc))\n",
    "    bri_value <- atanh(sqrt(rem_bri))\n",
    "    cal_value <- atanh(sqrt(rem_cal))\n",
    "    acc_value <- atanh(sqrt(rem_acc))\n",
    "    sens_value <- atanh(sqrt(rem_sens))\n",
    "    spec_value <- atanh(sqrt(rem_spec))    \n",
    "    r2_value <- atanh(sqrt(rem_r2))\n",
    "    \n",
    "    res <- c(auc_value, bri_value, cal_value, acc_value, sens_value, spec_value, r2_value)\n",
    "    return(res)     \n",
    "}\n",
    "\n",
    "auc_app_values <- vector(mode=\"numeric\", length=length(imp_data_files))\n",
    "auc_values <- vector(mode=\"numeric\", length=length(imp_data_files))\n",
    "bri_values <- vector(mode=\"numeric\", length=length(imp_data_files)) \n",
    "cal_values <- vector(mode=\"numeric\", length=length(imp_data_files))\n",
    "acc_values <- vector(mode=\"numeric\", length=length(imp_data_files))\n",
    "sens_values <- vector(mode=\"numeric\", length=length(imp_data_files))\n",
    "spec_values <- vector(mode=\"numeric\", length=length(imp_data_files))\n",
    "r2_values <- vector(mode=\"numeric\", length=length(imp_data_files))     \n",
    "\n",
    "N <- 0\n",
    "M <- length(imp_data_files) \n",
    "imp_iter <- 0\n",
    "for (fn in imp_data_files) {\n",
    "    imp_iter <- imp_iter + 1\n",
    "    train_data <- read.table(fn, sep=\",\", header=TRUE)    \n",
    "    N <- nrow(train_data)\n",
    "    yvalues <- train_data[yvar]\n",
    "    mod_formula <- as.formula(model_formula)\n",
    "    model <- glm(family=\"binomial\", formula=mod_formula, data=train_data)\n",
    "\n",
    "    prob <- predict(model, train_data)\n",
    "    pred <- prediction(prob, yvalues)\n",
    "    auc <- performance(pred, measure = \"auc\")\n",
    "    auc_app <- auc@y.values[[1]]\n",
    "    \n",
    "    bootres <- boot(train_data, optim, R=num_boot, parallel=\"multicore\", ncpus=4)\n",
    "        \n",
    "    auc_app_values[imp_iter] <- atanh(sqrt(auc_app))\n",
    "    auc_values[imp_iter] <- bootres$t[,1]\n",
    "    bri_values[imp_iter] <- bootres$t[,2]\n",
    "    cal_values[imp_iter] <- bootres$t[,3]\n",
    "    acc_values[imp_iter] <- bootres$t[,4]\n",
    "    sens_values[imp_iter] <- bootres$t[,5]\n",
    "    spec_values[imp_iter] <- bootres$t[,6]\n",
    "    r2_values[imp_iter] <- bootres$t[,7]\n",
    "}\n",
    "     \n",
    "auc_app_mean <- zinv(auc_app_values, N, M)\n",
    "auc_mean <- zinv(auc_values, N, M)\n",
    "bri_mean <- zinv(bri_values, N, M)\n",
    "cal_mean <- zinv(cal_values, N, M)\n",
    "acc_mean <- zinv(acc_values, N, M)\n",
    "sens_mean <- zinv(sens_values, N, M)\n",
    "spec_mean <- zinv(spec_values, N, M)\n",
    "r2_mean <- zinv(r2_values, N, M)\n",
    "\n",
    "print(sprintf(\"Apparent AUC : %0.2f (%0.2f, %0.2f)\", auc_app_mean[1], auc_app_mean[2], auc_app_mean[3]))\n",
    "print(sprintf(\"Corrected AUC: %0.2f (%0.2f, %0.2f)\", auc_mean[1], auc_mean[2], auc_mean[3]))\n",
    "print(sprintf(\"Adjusted R2  : %0.2f (%0.2f, %0.2f)\", r2_mean[1], r2_mean[2], r2_mean[3]))\n",
    "print(sprintf(\"Brier score  : %0.2f (%0.2f, %0.2f)\", bri_mean[1], bri_mean[2], bri_mean[3]))\n",
    "print(sprintf(\"Calibration  : %0.2f (%0.2f, %0.2f)\", cal_mean[1], cal_mean[2], cal_mean[3]))\n",
    "print(sprintf(\"Accuracy     : %0.2f (%0.2f, %0.2f)\", acc_mean[1], acc_mean[2], acc_mean[3]))   \n",
    "print(sprintf(\"Sensitivity  : %0.2f (%0.2f, %0.2f)\", sens_mean[1], sens_mean[2], sens_mean[3]))\n",
    "print(sprintf(\"Specificity  : %0.2f (%0.2f, %0.2f)\", spec_mean[1], spec_mean[2], spec_mean[3]))\n",
    "\n",
    "sink(paste0(model_folder, \"/boot.txt\"), append=FALSE, split=FALSE)\n",
    "print(sprintf(\"Apparent AUC : %0.2f (%0.2f, %0.2f)\", auc_app_mean[1], auc_app_mean[2], auc_app_mean[3]))\n",
    "print(sprintf(\"Corrected AUC: %0.2f (%0.2f, %0.2f)\", auc_mean[1], auc_mean[2], auc_mean[3]))\n",
    "print(sprintf(\"Adjusted R2  : %0.2f (%0.2f, %0.2f)\", r2_mean[1], r2_mean[2], r2_mean[3]))\n",
    "print(sprintf(\"Brier score  : %0.2f (%0.2f, %0.2f)\", bri_mean[1], bri_mean[2], bri_mean[3]))\n",
    "print(sprintf(\"Calibration  : %0.2f (%0.2f, %0.2f)\", cal_mean[1], cal_mean[2], cal_mean[3]))\n",
    "print(sprintf(\"Accuracy     : %0.2f (%0.2f, %0.2f)\", acc_mean[1], acc_mean[2], acc_mean[3]))   \n",
    "print(sprintf(\"Sensitivity  : %0.2f (%0.2f, %0.2f)\", sens_mean[1], sens_mean[2], sens_mean[3]))\n",
    "print(sprintf(\"Specificity  : %0.2f (%0.2f, %0.2f)\", spec_mean[1], spec_mean[2], spec_mean[3]))\n",
    "sink()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
